# 安置组PG

## 安置组

### 自动缩放安置组

放置组（PG）是Ceph分发数据的内部实现细节。您可以通过启用_pg-autoscaling_来允许集群根据集群的使用方式提出建议或自动调整PG 。

系统中的每个池有一个`pg_autoscale_mode`可以设置的财产`off`，`on`或`warn`。

* `off`：禁用此池的自动缩放。管理员可以为每个池选择适当的PG号。有关更多信息，请参考[选择放置组的数量](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#choosing-number-of-placement-groups)。
* `on`：启用给定池的PG计数的自动调整。
* `warn`：应调整PG计数时发出健康警报

要为现有池设置自动缩放模式，请执行以下操作：

```text
ceph osd pool set <pool-name> pg_autoscale_mode <mode>
```

例如，要在pool上启用自动缩放`foo`，请执行以下操作：

```text
ceph osd pool set foo pg_autoscale_mode on
```

您还可以使用以下方法配置`pg_autoscale_mode`应用于以后创建的任何池的默认值：

```text
ceph config set global osd_pool_default_pg_autoscale_mode <mode>
```

#### 查看PG缩放建议

您可以使用以下命令查看每个池，池的相对利用率以及对PG计数的任何建议更改：

```text
ceph osd pool autoscale-status
```

输出将类似于：

```text
POOL    SIZE  TARGET SIZE  RATE  RAW CAPACITY   RATIO  TARGET RATIO  EFFECTIVE RATIO PG_NUM  NEW PG_NUM  AUTOSCALE
a     12900M                3.0        82431M  0.4695                                     8         128  warn
c         0                 3.0        82431M  0.0000        0.2000           0.9884      1          64  warn
b         0        953.6M   3.0        82431M  0.0347                                     8              warn
```

**SIZE**是存储在池中的数据量。**TARGET SIZE**（如果存在）是管理员指定的期望最终存储在此池中的数据量。系统使用两个值中的较大者进行计算。

**RATE**是池的乘数，它确定要消耗多少原始存储容量。例如，3个副本池的比率为3.0，而ak = 4，m = 2擦除编码池的比率为1.5。

**原始容量**是OSD上负责存储该池（可能还有其他池）数据的原始存储容量的总量。 **比率**是该池消耗的总容量的比率（即比率=大小\*比率/原始容量）。

**TARGET RATIO**（如果存在）是管理员已指定他们希望该池相对于设置了目标比率的其他池消耗的存储比率。如果同时指定了目标大小字节和比率，则比率优先。

**有效比率**是通过两种方式进行调整后的目标比率：

1. 减去设置了目标大小的池预期使用的任何容量
2. 通过设置目标比率对池中的目标比率进行标准化，以便它们共同针对其余空间。例如，具有target\_ratio 1.0的4个池的有效比率为0.25。

系统使用实际比率和有效比率中的较大者进行计算。

**PG\_NUM**是池的当前PG数量（如果`pg_num` 正在进行更改，则为池正在使用的PG的当前数量）。 系统认为应该将**NEW PG\_NUM**`pg_num`更改为池。它始终是2的幂，并且仅在“理想”值与当前值的差异大于3时才存在。

最后一列，**AUTOSCALE**，是池`pg_autoscale_mode`，并将于要么`on`，`off`或`warn`。

#### 自动缩放

最简单的方法是允许群集根据使用情况自动扩展PG。Ceph将查看整个系统的PG的总可用存储量和目标数量，查看每个池中存储了多少数据，并尝试相应地分配PG。该系统的方法相对保守，仅当当前PG（`pg_num`）数量比其认为的数量多3倍时才对池进行更改。

每个OSD的PG的目标数量基于可 `mon_target_pg_per_osd`配置（默认值：100），可以通过以下方式进行调整：

```text
ceph config set global mon_target_pg_per_osd 100
```

自动缩放器将分析池并在每个子树的基础上进行调整。因为每个池可能映射到不同的CRUSH规则，并且每个规则可能在不同的设备之间分配数据，所以Ceph将考虑独立使用层次结构的每个子树。例如，映射到ssd类的OSD的池和映射到hdd类的OSD的池将分别具有最佳PG计数，具体取决于这些相应设备类型的数量。

#### 指定有望池大小

首次创建集群或池时，它将消耗集群总容量的一小部分，并且在系统中似乎只需要少量的放置组。但是，在大多数情况下，群集管理员会很好地了解哪些池将随时间消耗大部分系统容量。通过将此信息提供给Ceph，可以从一开始就使用更合适数量的PG，从而避免进行后续调整 `pg_num`以及在进行这些调整时与移动数据相关的开销。

池的_目标大小_可以通过两种方式指定：要么以池的绝对大小（即字节）为单位，要么以相对于具有一`target_size_ratio`组其他池的权重为单位。

例如，：

```text
ceph osd pool set mypool target_size_bytes 100T
```

会告诉系统mypool预计会占用100 TiB的空间。或者，：

```text
ceph osd pool set mypool target_size_ratio 1.0
```

会告诉系统mypool与`target_size_ratio`set 的其他池相比消耗1.0 。如果mypool是集群中唯一的池，则意味着预期使用了总容量的100％。如果第二个池的`target_size_ratio` 1.0，则两个池都将使用50％的群集容量。

您还可以在创建时使用命令的可选参数或参数设置池的目标大小。`--target-size-bytes <bytes>--target-size-ratio <ratio>ceph osd pool create`

请注意，如果指定了不可能的目标大小值（例如，容量大于整个群集的容量），则会发出健康警告（`POOL_TARGET_SIZE_BYTES_OVERCOMMITTED`）。

如果为池指定了`target_size_ratio`和`target_size_bytes`，则仅考虑比率，并且`POOL_HAS_TARGET_SIZE_BYTES_AND_RATIO`将发出运行状况警告（）。

#### 指定池的PG的边界

也可以为一个池指定最小数量的PG。这对于确定执行IO时客户端将看到的并行度的数量的下限很有用，即使池中大多数都是空的。设置下限可防止Ceph将PG编号减少（或建议减少）到配置的编号以下。

您可以使用以下方法设置池的最小PG数量：

```text
ceph osd pool set <pool-name> pg_num_min <num>
```

您还可以使用命令的可选参数在创建池时指定最小PG计数。`--pg-num-min <num>ceph osd pool create`

### PG\_NUM的预选

使用以下方法创建新池时：

```text
ceph osd pool create {pool-name} pg_num
```

必须选择的值，`pg_num`因为它不能（当前）自动计算。以下是一些常用的值：

* 少于5个OSD设置`pg_num`为128
* 5至10个OSD设置`pg_num`为512
* 10至50个OSD设置`pg_num`为1024
* 如果您有超过50个OSD，则需要了解折衷方案以及如何自己计算`pg_num`价值
* 要[自行](http://ceph.com/pgcalc/)计算`pg_num`值，请使用[pgcalc](http://ceph.com/pgcalc/)工具

随着OSD数量的增加，为pg\_num选择正确的值变得更加重要，因为它对群集的行为以及发生错误时的数据持久性（即，灾难性事件导致数据丢失）。

### 如何使用展示位置组？

放置组（PG）聚集了池中的对象，因为以每个对象为基础跟踪对象放置和对象元数据在计算上是昂贵的，即，具有数百万个对象的系统无法实际以每个对象为基础跟踪放置。

![](https://docs.ceph.com/docs/nautilus/_images/ditaa-1fde157d24b63e3b465d96eb6afea22078c85a90.png)

Ceph客户端将计算对象应位于哪个放置组中。它通过散列对象ID并基于已定义池中PG的数量和池ID来应用操作。有关详细信息，请参见将[PG映射到OSD](https://docs.ceph.com/docs/nautilus/architecture#mapping-pgs-to-osds)。

放置组中对象的内容存储在一组OSD中。例如，在大小为2的复制池中，每个放置组将在两个OSD上存储对象，如下所示。

![](https://docs.ceph.com/docs/nautilus/_images/ditaa-3c86866fb6edc99dad6ccf51e25e536806f0b079.png)

如果OSD＃2失败，则将另一个分配给放置组＃1，并用OSD＃1中所有对象的副本填充。如果池大小从两个更改为三个，则会将一个附加的OSD分配给该放置组，并将接收该放置组中所有对象的副本。

展示位置组不拥有OSD；他们与同一资源池甚至其他资源池中的其他展示位置组共享它。如果OSD＃2失败，则放置组＃2还必须使用OSD＃3恢复对象的副本。

当放置组的数量增加时，将为新的放置组分配OSD。CRUSH函数的结果也将更改，并且先前放置组中的某些对象将被复制到新的放置组中，并从旧的放置组中删除。

### 放置组权衡

数据持久性以及所有OSD之间的均匀分配都需要更多的放置组，但应将其数量减少到最少，以节省CPU和内存。

#### 数据持久性

OSD发生故障后，数据丢失的风险会增加，直到完全恢复其中包含的数据为止。让我们想象一下在单个放置组中导致永久性数据丢失的情况：

* OSD失败，并且它包含的对象的所有副本均丢失。对于放置组中的所有对象，副本的数量突然从三个减少到两个。
* Ceph通过选择一个新的OSD重新创建所有对象的第三个副本，开始对该放置组的恢复。
* 在同一放置组内的另一个OSD在新OSD完全填充第三份副本之前发生故障。这样，某些对象将只有一个幸存副本。
* Ceph选择了另一个OSD并保持复制对象以恢复所需的副本数。
* 在同一放置组中的第三个OSD在恢复完成之前发生故障。如果此OSD包含对象的唯一剩余副本，则它将永久丢失。

在三个副本池中包含10个OSD和512个放置组的群集中，CRUSH将为每个放置组提供三个OSD。最后，每个OSD将托管（512 \* 3）/ 10 =〜150个放置组。当第一个OSD发生故障时，以上情况将同时启动所有150个放置组的恢复。

正在恢复的150个放置组可能均匀分布在剩余的9个OSD上。因此，每个剩余的OSD可能会将对象的副本发送给所有其他对象，并且还可能会接收一些要存储的新对象，因为它们已成为新放置组的一部分。

完成恢复所需的时间完全取决于Ceph集群的架构。假设每个OSD由一台机器上的1TB SSD托管，并且所有OSD都连接到10Gb / s交换机，并且单个OSD的恢复在M分钟内完成。如果每台计算机使用不带SSD日志的微调器和1Gb / s开关的两个OSD，则速度至少要慢一个数量级。

在这种大小的群集中，放置组的数量几乎对数据持久性没有影响。可能是128或8192，恢复速度不会变慢或变快。

但是，将相同的Ceph集群增加到20个OSD而不是10个OSD可能会加快恢复速度，从而显着提高数据的持久性。现在，每个OSD只能参与约75个放置组，而不是只有10个OSD时的约150个放置组，并且仍然需要全部19个剩余OSD执行相同数量的对象副本才能恢复。但是，如果10个OSD必须每个复制大约100GB，则现在它们必须每个复制50GB。如果网络是瓶颈，恢复将以两倍的速度进行。换句话说，当OSD数量增加时，恢复速度会更快。

如果该群集增长到40个OSD，则每个OSD将仅托管约35个放置组。如果OSD死亡，则恢复将保持更快的速度，除非它被另一个瓶颈阻塞。但是，如果该群集增长到200个OSD，则每个OSD将仅托管约7个放置组。如果OSD死亡，则将在这些放置组中的最多约21（7 \* 3）个OSD之间进行恢复：恢复将比有40个OSD时花费的时间更长，这意味着应增加放置组的数量。

无论恢复时间有多短，第二个OSD在进行过程中都有可能发生故障。在上述10个OSD集群中，如果它们中的任何一个失败，那么〜17个放置组（即，正在恢复的大约150个/ 9个放置组）将只有一个幸存副本。并且，如果剩余的8个OSD中的任何一个发生故障，则两个放置组的最后一个对象很可能会丢失（即，〜17/8个放置组，仅恢复了一个剩余的副本）。

当群集的大小增加到20个OSD时，丢失三个OSD损坏的放置组的数量将减少。丢失的第二个OSD将降级〜4个（即，恢复的〜75个/ 19个放置组），而不是〜17个，而丢失的第三个OSD仅在它是包含尚存副本的四个OSD之一时才丢失数据。换句话说，如果在恢复时间范围内丢失一个OSD的概率为0.0001％，则它从具有10个OSD的群集中的17 \* 10 \* 0.0001％变为具有20个OSD的群集中的4 \* 20 \* 0.0001％。

简而言之，更多的OSD意味着恢复更快，而导致级联组永久丢失的级联故障的风险更低。就数据持久性而言，在少于50个OSD的群集中，具有512或4096个放置组大致等效。

注意：向群集添加的新OSD可能需要很长时间才能分配有分配给它的放置组。但是，不会降低任何对象的质量，也不会影响群集中包含的数据的持久性。

#### 池中的对象分布

理想情况下，对象均匀地分布在每个放置组中。由于CRUSH计算每个对象的放置组，但实际上不知道该放置组内每个OSD中存储了多少数据，因此放置组数与OSD数之比可能会显着影响数据的分布。

例如，如果在三个副本池中有一个用于十个OSD的放置组，则仅使用三个OSD，因为CRUSH别无选择。当有更多的放置组可用时，对象更有可能在其中均匀分布。CRUSH还尽一切努力在所有现有的展示位置组中平均分配OSD。

只要放置组比OSD多一个或两个数量级，则分布应该均匀。例如，用于3个OSD的256个放置组，用于10个OSD的512或1024个放置组等。

数据分布不均可能是由OSD与放置组之间的比率以外的因素引起的。由于CRUSH未考虑对象的大小，因此一些非常大的对象可能会造成不平衡。假设有100万个4K对象（总计4GB）均匀分布在10个OSD的1024个放置组中。他们将在每个OSD上使用4GB / 10 = 400MB。如果将一个400MB对象添加到池中，则支持放置对象的放置组的三个OSD将填充400MB + 400MB = 800MB，而其余七个将仅占据400MB。

#### 内存，CPU和网络使用率

对于每个放置组，OSD和MON始终需要内存，网络和CPU，并且在恢复期间甚至更多。通过对放置组内的对象进行聚类来共享此开销是它们存在的主要原因之一。

最小化放置组的数量可以节省大量资源。

### 选择展示位置组的数量

如果您有超过50个OSD，我们建议每个OSD大约有50-100个放置组，以平衡资源使用，数据持久性和分发。如果OSD少于50个，则最好在上述[预选](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#preselection)中进行[选择](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#preselection)。对于单个对象池，可以使用以下公式获取基线：

```text
             (OSDs * 100)
Total PGs =  ------------
              pool size
```

当**池大小**或者是复制品的复制池或K + M总和擦除编码池（如返回的数字**头孢OSD擦除码配置文件中看到**）。

然后，您应该检查结果是否与您设计Ceph集群的方式有意义，以最大程度地提高[数据持久性](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#data-durability)， [对象分配](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#object-distribution)并最小化[资源使用](https://docs.ceph.com/docs/nautilus/rados/operations/placement-groups/#resource-usage)。

结果应始终**四舍五入到最接近的2的幂**。

只有2的幂可以平衡放置组之间的对象数量。其他值将导致OSD上的数据分布不均。它们的使用应仅限于从两个方中的一个逐步增加到另一方。

例如，对于具有200个OSD和3个副本的池大小的群集，您可以按以下方式估计PG的数量：

```text
(200 * 100)
----------- = 6667. Nearest power of 2: 8192
     3
```

当使用多个数据池存储对象时，您需要确保在每个池的放置组数量与每个OSD的放置组数量之间取得平衡，以便获得合理的放置组总数，从而使每个OSD的方差很小而不会增加系统资源的负担或使对等进程太慢。

例如，一个10个池的群集，每个池在十个OSD上具有512个放置组，则总共有5120个放置组分布在十个OSD上，即每个OSD 512个放置组。那不会使用太多资源。但是，如果创建了1000个池，每个池有512个放置组，则OSD将分别处理约50,000个放置组，这将需要更多的资源和时间来进行对等。

您可能会发现[PGCalc](http://ceph.com/pgcalc/)工具很有帮助。

### 设置放置组数

要设置池中的放置组数量，必须在创建池时指定放置组的数量。有关详细信息，请参见[创建池](https://docs.ceph.com/docs/nautilus/rados/operations/pools#createpool)。即使在创建池之后，您也可以使用以下方法更改展示位置组的数量：

```text
ceph osd pool set {pool-name} pg_num {pg_num}
```

增加展示位置组的数量之后，还必须增加展示位置（`pgp_num`）的展示位置组的数量，群集才能重新平衡。该`pgp_num`会是将由CRUSH算法可考虑放置位置的组数。增加会`pg_num`拆分展示位置组，但在进行展示的展示位置组之前，数据不会迁移到较新的展示位置组。`pgp_num`增加。的值`pgp_num` 应等于`pg_num`。要增加用于放置的放置组的数量，请执行以下操作：

```text
ceph osd pool set {pool-name} pgp_num {pgp_num}
```

减少PG数量时，`pgp_num`将自动为您调整。

### 获取展示位置组的数量

要获取池中的展示位置组数，请执行以下操作：

```text
ceph osd pool get {pool-name} pg_num
```

### 获取集群的PG统计信息

要获取集群中展示位置组的统计信息，请执行以下操作：

```text
ceph pg dump [--format {format}]
```

有效格式为`plain`（默认）和`json`。

### 获取卡住的PG的统计信息

要获取所有处于指定状态的展示位置组的统计信息，请执行以下操作：

```text
ceph pg dump_stuck inactive|unclean|stale|undersized|degraded [--format <format>] [-t|--threshold <seconds>]
```

**不活动的**放置组无法处理读写，因为它们正在等待OSD包含最新数据。

**不干净的**放置组包含未复制所需次数的对象。他们应该正在恢复。

**陈旧的**放置组处于未知状态-承载它们的OSD暂时未向监视集群报告（由配置`mon_osd_report_timeout`）。

有效格式为`plain`（默认）和`json`。阈值定义了放置组停留在返回的统计信息中之前所停留的最小秒数（默认为300秒）。

### 获取PG地图

要获取特定放置组的放置组映射，请执行以下操作：

```text
ceph pg map {pg-id}
```

例如：

```text
ceph pg map 1.6c
```

Ceph将返回放置组图，放置组和OSD状态：

```text
osdmap e13 pg 1.6c (1.6c) -> up [1,0] acting [1,0]
```

### 获取PG统计信息

要检索特定展示位置组的统计信息，请执行以下操作：

```text
ceph pg {pg-id} query
```

### 清理一个放置组

要清理展示位置组，请执行以下操作：

```text
ceph pg scrub {pg-id}
```

Ceph检查主节点和任何副本节点，生成放置组中所有对象的目录并进行比较，以确保没有丢失或不匹配的对象，并且它们的内容一致。假设所有副本都匹配，则最终的语义扫描可确保所有与快照相关的对象元数据都是一致的。通过日志报告错误。

要清理特定池中的所有放置组，请执行以下操作：

```text
ceph osd pool scrub {pool-name}
```

### 优先安排展示位置组的回填/恢复

您可能会遇到这样的情况，即一堆展示位置组需要恢复和/或回填，并且某些特定的组保存的数据比其他的更为重要（例如，那些PG可能保存正在运行的机器使用的图像的数据，而其他PG可能由不活动的机器使用/较少的相关数据）。在这种情况下，您可能希望优先考虑恢复这些组，以便更早地恢复存储在这些组上的数据的性能和/或可用性。为此（在回填或恢复期间将特定的展示位置组标记为优先），请执行以下操作：

```text
ceph pg force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
ceph pg force-backfill {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

这将导致Ceph首先在其他展示位置组之前对指定的展示位置组执行恢复或回填。这不会中断当前正在进行的回填或恢复，但会导致尽快处理指定的PG。如果您改变主意或优先考虑错误的组，请使用：

```text
ceph pg cancel-force-recovery {pg-id} [{pg-id #2}] [{pg-id #3} ...]
ceph pg cancel-force-backfill {pg-id} [{pg-id #2}] [{pg-id #3} ...]
```

这将从这些PG中删除“ force”标志，并将以默认顺序对其进行处理。同样，这不会影响当前正在处理的展示位置组，只会影响仍在排队的展示位置组。

恢复或回填组后，将自动清除“ force”标志。

同样，您可以使用以下命令强制Ceph首先对指定池中的所有展示位置组执行恢复或回填：

```text
ceph osd pool force-recovery {pool-name}
ceph osd pool force-backfill {pool-name}
```

要么：

```text
ceph osd pool cancel-force-recovery {pool-name}
ceph osd pool cancel-force-backfill {pool-name}
```

如果您改变主意，则可以恢复到默认的恢复或回填优先级。

请注意，这些命令可能会破坏Ceph内部优先级计算的顺序，因此请谨慎使用！特别是，如果您有多个当前共享相同底层OSD的池，并且某些特定的池比其他池更重要，则建议您使用以下命令以更好的顺序重新排列所有池的恢复/回填优先级：

```text
ceph osd pool set {pool-name} recovery_priority {value}
```

例如，如果您有10个池，则可以将最重要的一个优先级设置为10，下一个9，等等。或者您可以不理会大多数池，而说3个重要的池分别设置为优先级1或优先级3、2、1。

### 还原丢失

如果群集丢失了一个或多个对象，并且您决定放弃对丢失数据的搜索，则必须将未找到的对象标记为`lost`。

如果已查询所有可能的位置并且仍然丢失了对象，则可能必须放弃丢失的对象。给定异常的故障组合，使集群能够了解在恢复写本身之前执行的写，这是可能的。

当前唯一受支持的选项是“还原”，它可以回滚到该对象的先前版本，或者（如果是新对象）则完全忘记它。要将“未找到”的对象标记为“丢失”，请执行以下操作：

```text
ceph pg {pg-id} mark_unfound_lost revert|delete
```

重要 

请谨慎使用此功能，因为它可能会使期望对象存在的应用程序感到困惑。

